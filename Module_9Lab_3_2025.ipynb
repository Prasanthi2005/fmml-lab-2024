{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prasanthi2005/fmml-lab-2024/blob/main/Module_9Lab_3_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCpbL40ggQf1"
      },
      "source": [
        "# Module 9: Convolutional Neural Networks\n",
        "## **Lab 3**\n",
        "### Module coordinator: Aswin Jose and Anirvinya Gururajan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hAW8ptqVeyP"
      },
      "source": [
        "## 1. Understanding Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6wfvhccKxWx"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/464/0*e-SMFTzO8r7skkpc\" width=650px/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZD5S7IQgHbU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BDE4WBHalreb"
      },
      "outputs": [],
      "source": [
        "# Importing some pytorch packages\n",
        "import torch\n",
        "from torch.nn import Conv2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpRXyTpVv7u"
      },
      "source": [
        "Central to CNNs, a convolution operation is a linear element-wise multiplication operation between a small filter/kernel and same-sized patch from the image. We move this filter over the image like a sliding window from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation. These filters can do simplest task like checking if there is a vertical line in the image or complicated task like detecting a human eye in the image.\n",
        "\n",
        "Let's look at the convolution formula:\n",
        "\n",
        "Convolution between image\n",
        "$f(x, y)$ and kernel $k(x, y)$ is\n",
        "$$f(x,y) * k(x,y) = \\sum \\limits _{i=0} ^{W-1} \\sum \\limits _{j=0} ^{H-1} f(i, j) k(x − i, y − j)$$\n",
        "\n",
        "where $W$ and $H$ are the the width and height of the image.\n",
        "\n",
        "The code demonstrates the convolution operation of a 2D matrix (image) with various filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amI6DTS0Ksvo"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Chaim-Baskin/publication/318849314/figure/fig1/AS:614287726870532@1523469015098/Image-convolution-with-an-input-image-of-size-7-7-and-a-filter-kernel-of-size-3-3.png\" alt=\"Convolution\" width=650px height=280px/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "IalqupPPkDil",
        "outputId": "6220fc07-9d4e-4d8b-8929-a74bd63bccda"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0], [1,1,0], [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0], [0,0,0], [1,1,1]])\n",
        "\n",
        "# On plotting the images\n",
        "plt.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()\n",
        "plt.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42INjCaketK",
        "outputId": "982e7717-8fea-425c-ba2d-e4b9464dd8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from first image:  3\n",
            "Output from second image:  0\n"
          ]
        }
      ],
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tba3ySYUk2df",
        "outputId": "32550d23-eba0-417b-c1a3-1c0a0af3d80d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from first image:  0\n",
            "Output from second image:  3\n"
          ]
        }
      ],
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmYcPhDgk_in"
      },
      "source": [
        "Non-zero output suggests that there is a vertical edge present in the first image and not present in the second image. Similarly, horizontal edge is detected in second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNdrDtAKqyj2"
      },
      "source": [
        "Let's define a function to use convolution layer from Pytorch and use our own kernel to detect edges in image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5fRJziBk3YB"
      },
      "outputs": [],
      "source": [
        "def apply_conv(image, kernel, padding=0, stride=1):\n",
        "\n",
        "  #--------IMAGE PREPROCESSING-------\n",
        "  image = torch.from_numpy(image)\n",
        "  # Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "  # --------------KERNEL-------------\n",
        "  kernel = torch.from_numpy(kernel.astype(np.float32))\n",
        "\n",
        "  # Pytorch requires kernel of shape (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  kernel = kernel.view((1,1,kernel.shape[0], kernel.shape[1]))\n",
        "\n",
        "  # ---------CONVOLUTION LAYER from Pytorch--------\n",
        "  conv = Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, padding=padding, stride=stride)\n",
        "\n",
        "  # Set the kernel weights in the convolution layer\n",
        "  conv.weight = torch.nn.Parameter(kernel)\n",
        "\n",
        "  # ---------APPLY CONVOLUTION--------\n",
        "  output = conv(input.float())\n",
        "  output_img = output.data.numpy()  # Tensor to back in numpy\n",
        "  output_img = output_img.reshape((-1, output_img.shape[-1])) # Reshape to 2D image\n",
        "\n",
        "  return output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1HPV6fFZloyc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "3934940b-b8f0-4541-d35b-c07da34253dd"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected np.ndarray (got NoneType)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-53b9a77372c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                    [ 1, 1, 1]])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m filter = np.array([[1,0,-1],\n",
            "\u001b[0;32m<ipython-input-6-33fe92fb693b>\u001b[0m in \u001b[0;36mapply_conv\u001b[0;34m(image, kernel, padding, stride)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#--------IMAGE PREPROCESSING-------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got NoneType)"
          ]
        }
      ],
      "source": [
        "# Our original lotus image\n",
        "image = cv2.imread('/content/grid1 (1).jpg', 0)\n",
        "\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "out1 = apply_conv(image, filter, padding=0, stride=1)\n",
        "\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "out2 = apply_conv(image, filter, padding=0, stride=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgwXwbUKnmEr"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.add_subplot(1,3,1)\n",
        "ax.imshow(image, cmap='gray')\n",
        "ax.set_title('Original Image')\n",
        "ax = fig.add_subplot(1,3,2)\n",
        "ax.set_title('Horizontal edge')\n",
        "ax.imshow(out1, cmap='gray')\n",
        "ax = fig.add_subplot(1,3,3)\n",
        "ax.imshow(out2, cmap='gray')\n",
        "ax.set_title('Vertical edge')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpA0yEk1BgRb"
      },
      "source": [
        "## Pooling Layers\n",
        "\n",
        "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.\n",
        "\n",
        "1) Max Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png' height=150px/>\n",
        "\n",
        "2) Average Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png' height=150px/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu3QIU7AEO_x"
      },
      "source": [
        "## Softmax layer/activation\n",
        "Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.\n",
        "\n",
        "Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
        "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg' height=170px />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6grxC0TKKSF"
      },
      "source": [
        "## Learning to train a CNN network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlO-uZUHnn_-"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnezCUbwGqzd"
      },
      "outputs": [],
      "source": [
        "#  Images returned from torchvision dataset classes is in range [0,1]\n",
        "# We transform them to tensors and normalize them to range [-1,1] using 'Normalize' transform\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2M57DhHGupn"
      },
      "outputs": [],
      "source": [
        "print('Training data shape : ', trainset.data.shape, len(trainset.targets))\n",
        "print('Testing data shape : ', testset.data.shape, len(testset.targets))\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_haw697lHCZs"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Wi6vW7IHcR"
      },
      "outputs": [],
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgxbRadcHIms"
      },
      "outputs": [],
      "source": [
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        # output layer 10 classes\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02meBxVOHLNL"
      },
      "outputs": [],
      "source": [
        "model = Net()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfKHypeYHNHO"
      },
      "outputs": [],
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuDnJL28HPKP"
      },
      "outputs": [],
      "source": [
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgKhwMrtHRCn"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM2wHKGuHToB"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "ax = fig.add_subplot(1,2, 2)\n",
        "ax.plot(np.arange(1,len(train_acc)+1),train_acc)\n",
        "plt.xlabel('Training accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Accuracy vs Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sHK9hhmI-VY"
      },
      "outputs": [],
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBQeCEB6REnH"
      },
      "source": [
        "## Questions\n",
        "1) List some reasons why we should prefer CNN over ANN for image classification?\n",
        "A.Here are some key reasons why Convolutional Neural Networks (CNNs) are preferred over Artificial Neural Networks (ANNs) for image classification:\n",
        " * Spatial Hierarchy and Feature Extraction:\n",
        "   * CNNs are designed to exploit the spatial structure present in images. Convolutional layers use filters that slide across the image, learning local patterns like edges, corners, and textures.\n",
        "   * Multiple convolutional layers stacked together can learn a hierarchy of features, where deeper layers combine simpler features to detect more complex objects or parts of objects.\n",
        "   * ANNs treat each pixel as an independent input, losing the inherent spatial relationships between neighboring pixels, which are crucial for understanding visual data.\n",
        " * Parameter Sharing and Reduced Complexity:\n",
        "   * CNNs utilize weight sharing, where the same filter is applied across different parts of the image. This drastically reduces the number of learnable parameters compared to a fully connected ANN.\n",
        "   * Fewer parameters lead to a more efficient model that requires less training data and is less prone to overfitting, especially with large, high-dimensional image datasets.\n",
        "   * ANNs, with their fully connected layers, have a massive number of parameters when dealing with images, making them computationally expensive and prone to overfitting.\n",
        " * Translation Invariance:\n",
        "   * The convolutional and pooling operations in CNNs help achieve translation invariance. This means that if an object appears in a different location in the image, the CNN can still recognize it because the learned features are detected regardless of their position.\n",
        "   * ANNs lack this inherent ability and would need to learn to recognize the same object in every possible location.\n",
        " * Local Connectivity:\n",
        "   * Neurons in a convolutional layer are only connected to a small, local region in the previous layer (the receptive field of the filter). This local connectivity helps in capturing local spatial dependencies.\n",
        "   * ANNs have fully connected layers where each neuron is connected to all neurons in the previous layer, which can be computationally expensive and doesn't inherently respect the local structure of images.\n",
        " * Dimensionality Reduction:\n",
        "   * Pooling layers in CNNs reduce the spatial dimensions of the feature maps, which helps in reducing the computational load and also makes the network more robust to small variations in the input.\n",
        "In essence, CNNs are specifically architected to understand and process the grid-like structure of image data in a way that traditional ANNs are not. This makes them significantly more effective, efficient, and robust for image classification tasks.\n",
        "\n",
        "\n",
        "2) Try improving the CNN performance further by tuning the hyperparameters(epochs, optimizer, LR etc). Report the improved test accuracy.\n",
        "A.Unfortunately, I cannot directly execute and train a CNN model to tune its hyperparameters and report improved test accuracy. Training a neural network requires a significant amount of computational resources, a suitable dataset, and a deep learning framework (like TensorFlow or PyTorch). I am an AI and do not have the capability to perform these actions in real-time.\n",
        "However, I can guide you on how you would typically approach hyperparameter tuning to improve CNN performance and what kind of improvements you might expect.\n",
        "General Steps for Hyperparameter Tuning:\n",
        " * Choose a Dataset: You would need a relevant image classification dataset (e.g., CIFAR-10, MNIST, a custom dataset).\n",
        " * Define a Baseline Model: Start with a basic CNN architecture. Train it with default or initial hyperparameter values to establish a baseline test accuracy.\n",
        " * Select Hyperparameters to Tune: Identify the most important hyperparameters that can significantly impact performance. Common ones include:\n",
        "   * Number of Epochs: How many times the entire training dataset is passed through the network.\n",
        "   * Learning Rate (LR): The step size at each iteration while moving towards a minimum of a loss function.\n",
        "   * Optimizer: The algorithm used to update the weights of the network (e.g., Adam, SGD, RMSprop).\n",
        "   * Batch Size: The number of samples processed before the model's weights are updated.\n",
        "   * Number of Filters in Convolutional Layers: The capacity of the convolutional layers to learn features.\n",
        "   * Kernel Size in Convolutional Layers: The spatial extent of the filters.\n",
        "   * Number of Layers: The depth of the network.\n",
        "   * Activation Functions: The non-linear functions applied after each layer (e.g., ReLU, LeakyReLU).\n",
        "   * Regularization Techniques: Methods to prevent overfitting (e.g., Dropout rate, L1/L2 regularization strength).\n",
        " * Choose a Tuning Strategy: Select a method to explore the hyperparameter space:\n",
        "   * Grid Search: Trying all possible combinations of a predefined set of hyperparameter values.\n",
        "   * Random Search: Sampling hyperparameter values randomly from a defined range. Often more efficient than grid search for a large number of hyperparameters.\n",
        "   * Bayesian Optimization: A more sophisticated method that uses past evaluation results to intelligently choose the next set of hyperparameters to try.\n",
        " * Evaluate Performance: For each set of hyperparameters, train the model on the training set (often with a validation split to monitor performance during training) and evaluate its performance on the test set.\n",
        " * Select the Best Hyperparameters: Choose the hyperparameter combination that yields the highest test accuracy (or other relevant metrics).\n",
        " * Retrain and Final Evaluation: Retrain the model with the best hyperparameters on the entire training set (or the combined training and validation sets) and report the final test accuracy.\n",
        "Potential Improvements and Expected Outcomes:\n",
        "By carefully tuning the hyperparameters, you can often see a significant improvement in the test accuracy of your CNN model compared to the baseline. The extent of the improvement depends on the initial architecture, the dataset, and the effectiveness of the tuning process.\n",
        "Here are some potential improvements you might observe:\n",
        " * Higher Accuracy: The primary goal is to increase the percentage of correctly classified images on the unseen test set. Improvements can range from a few percentage points to more substantial gains depending on how far the initial hyperparameters were from the optimal values.\n",
        " * Faster Convergence: Better learning rates and optimizers can lead to the model reaching a good performance level in fewer epochs, reducing training time.\n",
        " * Reduced Overfitting: Techniques like dropout and appropriate regularization strengths, when tuned correctly, can help the model generalize better to unseen data, leading to a smaller gap between training and test accuracy.\n",
        "Example Scenario (Illustrative - Not Executed):\n",
        "Let's say you started with a simple CNN and achieved a baseline test accuracy of 75% on the CIFAR-10 dataset with the following initial hyperparameters:\n",
        " * Epochs: 20\n",
        " * Optimizer: Adam (learning rate = 0.001)\n",
        " * Batch Size: 32\n",
        "After performing hyperparameter tuning using a method like random search, you might find that the following hyperparameters yield better performance:\n",
        " * Epochs: 50\n",
        " * Optimizer: Adam (learning rate = 0.0005)\n",
        " * Batch Size: 64\n",
        " * Added Dropout (rate = 0.2) in the fully connected layers.\n",
        "With these tuned hyperparameters, after retraining the model, you might observe an improved test accuracy of 82%.\n",
        "In conclusion, while I cannot provide a specific improved test accuracy without running experiments, the process of hyperparameter tuning is crucial for maximizing the performance of a CNN. By systematically exploring different hyperparameter values, you can often achieve significant gains in accuracy and generalization ability.\n",
        "\n",
        "3) What happens if you reduce the number of convolution layers to only 1?\n",
        "A.When you reduce the number of convolutional layers in a CNN to only one, you will likely observe a significant impact on the network's performance, particularly for complex image classification tasks. Here's a breakdown of what happens and why:\n",
        "Limitations of a Single Convolutional Layer:\n",
        " * Limited Feature Hierarchy: Deep CNNs excel because they learn a hierarchy of features. The first layers learn basic features like edges and corners. Subsequent layers combine these features to learn more complex patterns like shapes, textures, and object parts. With only one convolutional layer, the network can only learn very basic, low-level features directly from the input image. It lacks the capacity to abstract and combine these features into more meaningful representations needed to distinguish between complex objects or categories.\n",
        " * Reduced Complexity and Representational Power: A single convolutional layer has a limited number of learnable parameters and a shallow architecture. This significantly reduces the network's capacity to learn intricate relationships within the image data. It might be sufficient for very simple tasks with easily distinguishable features (e.g., distinguishing between horizontal and vertical lines), but it will struggle with real-world image classification datasets.\n",
        " * Inability to Capture High-Level Semantics: High-level semantic information about objects (e.g., \"this is a cat because it has ears, fur, and a tail in a certain arrangement\") is typically learned by deeper layers that integrate features from earlier layers. A single convolutional layer cannot capture these complex semantic relationships.\n",
        " * Poor Invariance to Variations: While a single convolutional layer with a filter can provide some degree of translation invariance for basic features, it won't be as robust to other variations like scale, rotation, or changes in viewpoint. Deeper networks learn to become more invariant to these transformations by learning increasingly abstract features.\n",
        " * Increased Reliance on Fully Connected Layers: To compensate for the lack of feature extraction in multiple convolutional layers, the subsequent fully connected layers would have to learn directly from the relatively unprocessed output of the single convolutional layer. This would require a much larger number of parameters in the fully connected layers, making the network prone to overfitting, especially with limited data. It also defeats one of the primary advantages of CNNs over ANNs, which is parameter efficiency through convolutional layers.\n",
        "Potential Outcomes:\n",
        " * Significantly Lower Accuracy: For most standard image classification datasets (CIFAR-10, ImageNet, etc.), the test accuracy will be drastically lower compared to a CNN with multiple layers. The model will likely underfit the data, meaning it won't be able to learn the underlying patterns effectively.\n",
        " * Faster Training (Potentially): With fewer layers and potentially fewer overall parameters (depending on the size of the fully connected layers), the training time per epoch might be faster. However, the model will likely converge to a much lower performance level.\n",
        " * Limited Generalization: The model will likely not generalize well to unseen data due to its inability to learn robust and abstract features.\n",
        "In Summary:\n",
        "Reducing the number of convolutional layers to just one severely limits the CNN's ability to learn complex, hierarchical features from images. This drastically reduces its representational power and its performance on most image classification tasks. While it might be sufficient for extremely simple visual tasks, it negates the key advantages of using a deep convolutional architecture for image analysis. You would almost certainly see a substantial drop in test accuracy compared to a deeper CNN.\n",
        "\n",
        "4) Why didn't we use the Softmax activation in the last layer of CNN?\n",
        "A.It's highly likely that you did use the Softmax activation function in the last layer of your CNN, especially if your task was multi-class classification. Softmax is the standard activation function for the output layer in such scenarios.\n",
        "However, if you encountered a situation where Softmax wasn't used in the final layer of a CNN for an image-related task, it would likely be for a different type of problem or a specific reason. Here are some possibilities:\n",
        "Scenarios where Softmax might NOT be used in the final layer of a CNN:\n",
        " * Binary Classification: If the task is to classify images into only two classes (e.g., cat vs. dog), the final layer typically has a single neuron with a Sigmoid activation function. The output of the sigmoid is a probability between 0 and 1, representing the likelihood of the input belonging to one of the classes.\n",
        " * Multi-label Classification: In this scenario, an image can belong to multiple classes simultaneously (e.g., an image containing both a \"cat\" and a \"dog\"). The final layer would typically have multiple neurons, each with a Sigmoid activation function. Each neuron's output represents the probability of the image belonging to a specific class, independently of the others.\n",
        " * Regression Tasks: If the goal is to predict a continuous value (e.g., predicting the age of a person in an image, or bounding box coordinates), the final layer would usually have one or more neurons with no activation function or a linear activation.\n",
        " * Feature Embedding: In some cases, the CNN might be used as a feature extractor, and the output of the last convolutional or pooling layer (before any fully connected layers) might be used as a feature vector for other tasks like similarity search or clustering. In this case, there might not be a final classification layer with Softmax.\n",
        " * Specific Loss Functions: Certain loss functions might implicitly handle the normalization and probability distribution aspects that Softmax provides. However, this is less common for standard classification tasks.\n",
        " * Error or Unusual Architecture: It's also possible that the absence of Softmax in the final layer of a classification CNN is an error in the design or implementation, which would likely lead to poor performance.\n",
        "Why Softmax is Typically Preferred for Multi-Class Classification:\n",
        " * Probability Distribution: Softmax converts the raw output scores (logits) from the previous layer into a probability distribution over all the classes. The output of each neuron in the Softmax layer represents the probability of the input image belonging to that specific class, and the sum of these probabilities across all classes is always 1.\n",
        " * Clear Interpretation: The probabilistic output of Softmax is easy to interpret, providing a confidence score for each class.\n",
        " * Compatibility with Cross-Entropy Loss: Softmax is naturally paired with the categorical cross-entropy loss function (or its sparse version). The cross-entropy loss measures the dissimilarity between the predicted probability distribution and the true one-hot encoded label. This combination is very effective for training multi-class classifiers.\n",
        "In summary, while Softmax is the standard for the final layer of a CNN in multi-class classification tasks due to its ability to produce a probability distribution, other activation functions (like Sigmoid for binary or multi-label) or no activation (for regression or feature embedding) are used depending on the specific problem being solved. If you encountered a multi-class CNN without Softmax in the final layer, it's worth investigating the specific task and architecture to understand the reasoning behind that choice.\n",
        "\n",
        "5) How does pooling help in this context ?\n",
        "A.In the context of Convolutional Neural Networks (CNNs) for image analysis, pooling plays several crucial roles that contribute significantly to the network's effectiveness and efficiency. Here's how pooling helps:\n",
        " * Reduced Spatial Dimensionality: Pooling layers downsample the feature maps produced by the convolutional layers. By reducing the spatial size (width and height) of these maps, pooling decreases the number of parameters in the subsequent layers (especially fully connected layers). This helps to:\n",
        "   * Reduce computational cost: Fewer parameters mean faster training and inference times.\n",
        "   * Control overfitting: By reducing the number of parameters, the model becomes less likely to memorize the training data and generalizes better to unseen data.\n",
        " * Translation Invariance (to a degree): Pooling makes the network more robust to small translations (shifts) in the input image. For example, if a feature (like an edge or a texture) is detected slightly shifted in the input, the max-pooling operation will still likely pick up the same prominent feature within its pooling window. This provides a degree of invariance to where the feature is located.\n",
        " * Focus on the Most Important Features:\n",
        "   * Max Pooling: The most common type of pooling, max pooling, selects the maximum value within each pooling window. This helps to retain the most salient or important features detected by the convolutional filters. It essentially discards less important activations and focuses on the strong responses, making the network more robust to noise and minor variations.\n",
        "   * Average Pooling: While less common than max pooling in deeper CNNs, average pooling calculates the average value within each pooling window. It provides a smoother downsampling and can retain more information about the overall presence of a feature, rather than just its strongest activation.\n",
        " * Increased Receptive Field for Subsequent Layers: As pooling reduces the spatial size of the feature maps, the neurons in the subsequent convolutional layers effectively have a larger receptive field in the original input image. This allows them to learn more global and complex features by considering information from a wider area of the previous layer.\n",
        " * Abstraction and Hierarchy: By progressively reducing the spatial dimensions and focusing on the most important features, pooling contributes to the hierarchical learning process in CNNs. Early convolutional layers detect low-level features, and as the data passes through pooling and subsequent convolutional layers, the network learns increasingly abstract and complex representations of the image.\n",
        "In essence, pooling helps to make CNNs more efficient, robust to variations in the input, and capable of learning hierarchical representations by summarizing feature maps and reducing their dimensionality. It plays a critical role in enabling CNNs to generalize well and achieve high performance on image-related tasks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}